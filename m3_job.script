#!/bin/bash
# Usage: sbatch slurm-openmp-job-script
# Prepared By: Kai Xi,  Apr 2015
#              help@massive.org.au

# NOTE: To activate a SLURM option, remove the whitespace between the '#' and 'SBATCH'

# To give your job a name, replace "MyJob" with an appropriate name
#SBATCH --job-name=NACO-Elias2-24-negfc_gb

# To set a project account for credit charging,
#SBATCH --account=pd87

# Request CPU resource for a openmp job, suppose it is a 12-thread job
#SBATCH --ntasks=1            # multi-processing
#SBATCH --ntasks-per-node=1   # to spread across multiple nodes, depending what is available with show_cluster
#SBATCH --cpus-per-task=16     # the number of CPUs, multiplied by ntasks

# Memory usage (MB)
#SBATCH --mem-per-cpu=2000

# Set your minimum acceptable wall time, format: day-hours:minutes:seconds
#SBATCH --time=7-0:00:00

# SBATCH --partition=m3i        # select a specific partition

# To receive an email when job completes or fails
#SBATCH --mail-user=<iain.hammond@monash.edu>
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL

# Set the file for output (stdout)
#SBATCH --output=pipeline_output%j.out

# Set the file for error log (stderr)
#SBATCH --error=pipeline_output_error%j.err

# Use reserved node to run job when a node reservation is made for you already
# SBATCH --reservation=reservation_name

# Command to run a openmp job
# Set OMP_NUM_THREADS to the same value as --cpus-per-task. One thread is best for CPU intensive tasks, more can actually slow it down
# env # print environmental variables if you wish, good for debugging
export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=$OMP_NUM_THREADS
export MKL_NUM_THREADS=$OMP_NUM_THREADS
source /home/ihammond/miniconda3/etc/profile.d/conda.sh # initialise conda shell
conda activate VIPenv # activate personal VIP conda environment
# ulimit -s unlimited # recommended by support team to fix stack size memory error (don't know if it actually works)
python run_script.py # runs the script
